{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "from tensorflow.keras.layers import Embedding,Dense,Dropout"
      ],
      "metadata": {
        "id": "whGwVQ4CsaLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "cNjjSfzKuGSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import Model"
      ],
      "metadata": {
        "id": "XjO4ik6ksaIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "Sx87u-8ssaFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "7YwV06i-saDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = []\n",
        "outputs = []\n",
        "\n",
        "data_file = open('spa.txt', encoding='utf-8')\n",
        "\n",
        "count = 0\n",
        "for line in data_file:\n",
        "    count += 1\n",
        "    if count > 20000:\n",
        "        break\n",
        "    if '\\t' not in line:\n",
        "        continue\n",
        "    ip, temp_op, extra = line.rstrip().split('\\t')\n",
        "    op = temp_op\n",
        "    inputs.append(ip)\n",
        "    outputs.append(op)"
      ],
      "metadata": {
        "id": "dgMN8KVusaAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs=[sentence.lower() for sentence in inputs]\n",
        "outputs=[sentence.lower() for sentence in outputs]"
      ],
      "metadata": {
        "id": "_WBcET2asZ-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = ['<start> '+sentence+' <end>' for sentence in outputs]"
      ],
      "metadata": {
        "id": "9-cSFuFasZ6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_sentences(sentences):\n",
        "    cleaned_sentences = []\n",
        "\n",
        "    def replace_non_alphabetic_with_null(input_string):\n",
        "        return re.sub(r'[^a-zA-Z]', '', input_string)\n",
        "\n",
        "    for sentence in sentences:\n",
        "        cleaned_sentence = replace_non_alphabetic_with_null(sentence)\n",
        "        cleaned_sentences.append(cleaned_sentence)\n",
        "\n",
        "    return cleaned_sentences"
      ],
      "metadata": {
        "id": "tcQLumpsIZ1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs[10:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyOk14TWJZV1",
        "outputId": "29a8db02-397c-486a-e9ca-c8821ebcbf70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['who?',\n",
              " 'wow!',\n",
              " 'fire!',\n",
              " 'fire!',\n",
              " 'fire!',\n",
              " 'help!',\n",
              " 'help!',\n",
              " 'help!',\n",
              " 'jump!',\n",
              " 'jump.']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs=clean_sentences(inputs)"
      ],
      "metadata": {
        "id": "ROg_42KiJRtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs[10:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "734debhpJdoe",
        "outputId": "d7090909-e925-4e80-b3f4-90a6566198bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['who', 'wow', 'fire', 'fire', 'fire', 'help', 'help', 'help', 'jump', 'jump']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EnglishTokenizer=Tokenizer(oov_token=\"<UNK>\")\n",
        "EnglishTokenizer.fit_on_texts(inputs)\n",
        "inp_sequences=EnglishTokenizer.texts_to_sequences(inputs)\n",
        "max_inp_len=max(len(i) for i in inp_sequences)\n",
        "src_sequences=pad_sequences(inp_sequences,maxlen=max_inp_len,padding=\"post\")\n",
        "Englishword2index=EnglishTokenizer.word_index\n",
        "Englishindex2word=EnglishTokenizer.index_word"
      ],
      "metadata": {
        "id": "S8A0M4A8sZ4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SpanishTokenizer=Tokenizer(oov_token=\"<UNK>\")\n",
        "SpanishTokenizer.fit_on_texts(outputs)\n",
        "op_sequences=SpanishTokenizer.texts_to_sequences(outputs)\n",
        "max_tar_len=max(len(i) for i in op_sequences)\n",
        "tar_sequences=pad_sequences(op_sequences,maxlen=max_tar_len,padding=\"post\")\n",
        "Spanishword2index=SpanishTokenizer.word_index\n",
        "Spanishindex2word=SpanishTokenizer.index_word"
      ],
      "metadata": {
        "id": "7gLenHB2sZ1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_vocab_size=len(Englishword2index)+1\n",
        "trg_vocab_size=len(Spanishword2index)+1\n",
        "print(\"src_vocab_size:\",src_vocab_size)\n",
        "print(\"tar_vocab_size:\",trg_vocab_size)"
      ],
      "metadata": {
        "id": "fPL6iVxXsZzP",
        "outputId": "21539cf8-1a24-467e-9372-15c11b021706",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src_vocab_size: 14920\n",
            "tar_vocab_size: 7853\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"max_inp_len:\",max_inp_len)\n",
        "print(\"max_tar_len:\",max_tar_len)"
      ],
      "metadata": {
        "id": "bguSvDQgsZvf",
        "outputId": "3e8d8f36-d5c0-4791-bae1-1a4522b6c416",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max_inp_len: 1\n",
            "max_tar_len: 14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionEmbedding(tf.keras.layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, hidden_dim):\n",
        "        super(PositionEmbedding, self).__init__()\n",
        "        word_embedding_matrix = self.get_position_encoding(vocab_size, hidden_dim)\n",
        "        position_embedding_matrix = self.get_position_encoding(sequence_length, hidden_dim)\n",
        "        self.word_embedding_layer = Embedding(\n",
        "            input_dim=vocab_size, output_dim=hidden_dim,\n",
        "            weights=[word_embedding_matrix],\n",
        "            trainable=False\n",
        "        )\n",
        "        self.position_embedding_layer = Embedding(\n",
        "            input_dim=sequence_length, output_dim=hidden_dim,\n",
        "            weights=[position_embedding_matrix],\n",
        "            trainable=False\n",
        "        )\n",
        "\n",
        "    def get_position_encoding(self, seq_len, d, n=10000):\n",
        "        P = np.zeros((seq_len, d))\n",
        "        for k in range(seq_len):\n",
        "            for i in np.arange(int(d/2)):\n",
        "                denominator = np.power(n, 2*i/d)\n",
        "                P[k, 2*i] = np.sin(k/denominator)\n",
        "                P[k, 2*i+1] = np.cos(k/denominator)\n",
        "        return P\n",
        "\n",
        "\n",
        "    def call(self, inputs):\n",
        "        position_indices = tf.range(tf.shape(inputs)[-1])\n",
        "        embedded_words = self.word_embedding_layer(inputs)\n",
        "        embedded_indices = self.position_embedding_layer(position_indices)\n",
        "        return embedded_words + embedded_indices"
      ],
      "metadata": {
        "id": "IXMVWMB6sZtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product(q,k,v,mask=None):\n",
        "    scale=q.shape[-1]\n",
        "    k=tf.transpose(k,perm=(0,1,3,2))\n",
        "    scaled=tf.matmul(q,k)/np.sqrt(scale)\n",
        "    if mask is not None:\n",
        "        scaled+=mask\n",
        "    attention_wts=tf.nn.softmax(scaled,axis=-1)\n",
        "    values=tf.matmul(scaled,v)\n",
        "    return values,attention_wts"
      ],
      "metadata": {
        "id": "2As8-P0DsZpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHead_Attention(tf.keras.layers.Layer):\n",
        "    def __init__(self,no_of_heads,d_model):\n",
        "        super(MultiHead_Attention,self).__init__()\n",
        "        self.n_heads=no_of_heads\n",
        "        self.head_dims=d_model//no_of_heads\n",
        "        self.qkv=Dense(3*d_model)\n",
        "        self.dense=Dense(d_model)\n",
        "\n",
        "    def call(self,inputs,mask=None):\n",
        "        batch_size,max_inp_len,d_model=inputs.shape\n",
        "        qkv=self.qkv(inputs)\n",
        "        qkv=tf.reshape(qkv,(tf.shape(inputs)[0], max_inp_len, self.n_heads, 3 * self.head_dims))\n",
        "        qkv=tf.transpose(qkv,perm=(0,2,1,3))\n",
        "        q,k,v=tf.split(qkv,num_or_size_splits=3,axis=3)\n",
        "        v,attention_wts=scaled_dot_product(q,k,v)\n",
        "        v=tf.reshape(v,(tf.shape(inputs)[0],max_inp_len,self.n_heads*self.head_dims))\n",
        "        output=self.dense(v)\n",
        "        return output"
      ],
      "metadata": {
        "id": "-AnpOAWCsZni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(tf.keras.layers.Layer):\n",
        "    def __init__(self,d_model,ffc,dropout):\n",
        "        super(FeedForward,self).__init__()\n",
        "        self.dense1=Dense(ffc,activation=\"relu\")\n",
        "        self.dense2=Dense(d_model)\n",
        "        self.dropout=Dropout(rate=dropout)\n",
        "\n",
        "    def call(self,inputs,training=False):\n",
        "        x=self.dense1(inputs)\n",
        "        x=self.dropout(x,training=training)\n",
        "        x=self.dense2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "DATJGXACuXED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNormalization(tf.keras.layers.Layer):\n",
        "    def __init__(self, parameter_shape, eps=1e-5):\n",
        "        super(LayerNormalization, self).__init__()\n",
        "        self.parameter_shape = parameter_shape\n",
        "        self.eps = eps\n",
        "        self.gamma = self.add_weight(\"gamma\", shape=parameter_shape, initializer=\"ones\", trainable=True)\n",
        "        self.beta = self.add_weight(\"beta\", shape=parameter_shape, initializer=\"zeros\", trainable=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        mean = tf.reduce_mean(inputs, axis=-1, keepdims=True)\n",
        "        var = tf.reduce_mean(tf.square(inputs - mean), axis=-1, keepdims=True)\n",
        "        std = tf.sqrt(var + self.eps)\n",
        "        y = (inputs - mean) / std\n",
        "        out = self.gamma * y + self.beta\n",
        "        return out"
      ],
      "metadata": {
        "id": "0d1MGYG9uXLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder_Layer(tf.keras.layers.Layer):\n",
        "    def __init__(self,num_of_heads,d_model,dff,dropout):\n",
        "        super(Encoder_Layer,self).__init__()\n",
        "        self.mha=MultiHead_Attention(num_of_heads,d_model)\n",
        "        self.dropout1=Dropout(rate=dropout)\n",
        "        self.ln1=LayerNormalization(d_model)\n",
        "        self.ff=FeedForward(d_model,dff,dropout)\n",
        "        self.dropout2=Dropout(rate=dropout)\n",
        "        self.ln2=LayerNormalization(d_model)\n",
        "\n",
        "    def call(self,inputs,mask,training):\n",
        "        mha_output=self.mha(inputs,mask)\n",
        "        output_dropout1=self.dropout1(mha_output,training=training)\n",
        "        ln1_output=self.ln1(output_dropout1+inputs)\n",
        "\n",
        "        ffn_output=self.ff(ln1_output,training)\n",
        "        ffn_output_dropout=self.dropout2(ffn_output,training=training)\n",
        "        ln2_output=self.ln2(ffn_output_dropout+ln1_output)\n",
        "        return ln2_output"
      ],
      "metadata": {
        "id": "vuOlLoLruXNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self,inp_seq_len, inp_vocab_size,embed_dim,num_of_heads,d_model,dff,dropout,num_layers):\n",
        "        super(Encoder,self).__init__()\n",
        "        self.embedding_layer = PositionEmbedding(inp_seq_len, inp_vocab_size,embed_dim)\n",
        "        self.encoder_layer=[Encoder_Layer(num_of_heads,d_model,dff,dropout) for _ in range(num_layers)]\n",
        "\n",
        "    def call(self,inputs,mask=False,training=False):\n",
        "        x=self.embedding_layer(inputs)\n",
        "        for layer in self.encoder_layer:\n",
        "            x=layer(x,mask,training)\n",
        "        return x"
      ],
      "metadata": {
        "id": "3l1HGFQBuXQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoder"
      ],
      "metadata": {
        "id": "dE4sCexvuon1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedMultiHead_Attention(tf.keras.layers.Layer):\n",
        "    def __init__(self,no_of_heads,d_model):\n",
        "        super(MaskedMultiHead_Attention,self).__init__()\n",
        "        self.n_heads=no_of_heads\n",
        "        self.head_dims=d_model//no_of_heads\n",
        "        self.qkv=Dense(3*d_model)\n",
        "        self.dense=Dense(d_model)\n",
        "\n",
        "    def call(self,inputs,mask=None):\n",
        "        batch_size,max_inp_len,d_model=inputs.shape\n",
        "        qkv=self.qkv(inputs)\n",
        "        desired_shape = (tf.shape(inputs)[0], max_inp_len, self.n_heads, 3 * self.head_dims)\n",
        "        qkv=tf.reshape(qkv,desired_shape)\n",
        "        qkv=tf.transpose(qkv,perm=(0,2,1,3))\n",
        "        q,k,v=tf.split(qkv,num_or_size_splits=3,axis=3)\n",
        "        v,attention_wts=scaled_dot_product(q,k,v,mask)\n",
        "        v=tf.reshape(v,(tf.shape(inputs)[0],max_inp_len,self.n_heads*self.head_dims))\n",
        "        output=self.dense(v)\n",
        "        return output"
      ],
      "metadata": {
        "id": "DbGD35PWuXSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadCross_Attention(tf.keras.layers.Layer):\n",
        "    def __init__(self,no_of_heads,d_model):\n",
        "        super(MultiHeadCross_Attention,self).__init__()\n",
        "        self.n_heads=no_of_heads\n",
        "        self.head_dims=d_model//no_of_heads\n",
        "        self.kv=Dense(2*d_model)\n",
        "        self.q=Dense(d_model)\n",
        "        self.dense=Dense(d_model)\n",
        "\n",
        "    def call(self,x,y,mask=None):\n",
        "        batch_size,max_inp_len,d_model=x.shape\n",
        "        batch_size,max_tar_len,d_model=y.shape\n",
        "        kv=self.kv(x)\n",
        "        q=self.q(y)\n",
        "        kv=tf.reshape(kv,(tf.shape(x)[0], max_inp_len, self.n_heads, 2 * self.head_dims))\n",
        "        q=tf.reshape(q,(tf.shape(y)[0], max_tar_len, self.n_heads, self.head_dims))\n",
        "        kv=tf.transpose(kv,perm=(0,2,1,3))\n",
        "        q=tf.transpose(q,perm=(0,2,1,3))\n",
        "        k,v=tf.split(kv,num_or_size_splits=2,axis=3)\n",
        "        v,attention_wts=scaled_dot_product(q,k,v,mask)\n",
        "        v=tf.reshape(v,(tf.shape(x)[0],max_tar_len,d_model))\n",
        "        output=self.dense(v)\n",
        "        return output"
      ],
      "metadata": {
        "id": "rgyizy1VuXWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder_Layer(tf.keras.layers.Layer):\n",
        "    def __init__(self,d_model,dropout,no_heads,dff):\n",
        "        super(Decoder_Layer,self).__init__()\n",
        "        self.mha=MaskedMultiHead_Attention(no_heads,d_model)\n",
        "        self.dropout1=Dropout(rate=dropout)\n",
        "        self.norm1=LayerNormalization(d_model)\n",
        "\n",
        "        self.mhca=MultiHeadCross_Attention(no_heads,d_model)\n",
        "        self.dropout2=Dropout(rate=dropout)\n",
        "        self.norm2=LayerNormalization(d_model)\n",
        "\n",
        "        self.ff=FeedForward(d_model,dff,dropout)\n",
        "        self.dropout3=Dropout(rate=dropout)\n",
        "        self.norm3=LayerNormalization(d_model)\n",
        "\n",
        "    def call(self,x,y,look_ahead_mask,padding_mask,training=False):\n",
        "        mha_output=self.mha(y,mask=look_ahead_mask)\n",
        "        mha_output_dropout=self.dropout1(mha_output,training=training)\n",
        "        norm1_output=self.norm1(mha_output_dropout+y)\n",
        "\n",
        "        cmha_output=self.mhca(x,norm1_output,mask=padding_mask)\n",
        "        cmha_output_dropout=self.dropout2(cmha_output,training=training)\n",
        "        norm2_output=self.norm2(cmha_output_dropout+norm1_output)\n",
        "\n",
        "        ff_output=self.ff(norm2_output,training)\n",
        "        ff_output_dropout=self.dropout3(ff_output,training=training)\n",
        "        norm3_output=self.norm3(ff_output_dropout+norm2_output)\n",
        "        return norm3_output"
      ],
      "metadata": {
        "id": "3_WavftkuXZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self,tar_seq_len, tar_vocab_size,embed_dim,n_layers,d_model,dropout,no_heads,dff):\n",
        "        super(Decoder,self).__init__()\n",
        "        self.embedding_layer = PositionEmbedding(tar_seq_len, tar_vocab_size,embed_dim)\n",
        "        self.decoder_layers=[Decoder_Layer(d_model,dropout,no_heads,dff) for _ in range(n_layers)]\n",
        "\n",
        "    def call(self,x,y,look_ahead_mask=None,padding_mask=None,training=False):\n",
        "        y=self.embedding_layer(y)\n",
        "        for layer in self.decoder_layers:\n",
        "            y=layer(x,y,look_ahead_mask,padding_mask,training)\n",
        "        return y"
      ],
      "metadata": {
        "id": "opnXjPPbuzMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self,inp_seq_len,inp_vocab_size,tar_seq_len, tar_vocab_size,embed_dim,n_layers,dropout,dff,n_heads,d_model):\n",
        "        super(Transformer,self).__init__()\n",
        "        self.encoder=Encoder(inp_seq_len, inp_vocab_size,embed_dim,n_heads,d_model,dff,dropout,n_layers)\n",
        "        self.decoder=Decoder(tar_seq_len, tar_vocab_size,embed_dim,n_layers,d_model,dropout,n_heads,dff)\n",
        "        self.dense=Dense(tar_vocab_size)\n",
        "\n",
        "    def create_padding_mask(self,seq):\n",
        "        seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "        return seq[:,tf.newaxis,tf.newaxis,:]\n",
        "\n",
        "    def create_look_ahead_mask(self,size):\n",
        "        mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "        return mask\n",
        "\n",
        "    def create_masks(self,inputs, target):\n",
        "        enc_padding_mask = self.create_padding_mask(inputs)\n",
        "        dec_padding_mask = self.create_padding_mask(target)\n",
        "        look_ahead_mask = self.create_look_ahead_mask(tf.shape(target)[1])\n",
        "        look_ahead_mask = tf.maximum(dec_padding_mask, look_ahead_mask)\n",
        "\n",
        "        return enc_padding_mask, look_ahead_mask, dec_padding_mask\n",
        "\n",
        "    def call(self,inputs,training=False):\n",
        "        x,y=inputs\n",
        "        enc_padding_mask,lookahead_mask,dec_padding_mask=self.create_masks(x,y)\n",
        "        encoder_op=self.encoder(x,mask=enc_padding_mask,training=training)\n",
        "        decoder_op=self.decoder(encoder_op,y,look_ahead_mask=lookahead_mask,\n",
        "                            padding_mask=enc_padding_mask,training=training)\n",
        "        output=self.dense(decoder_op)\n",
        "        return output"
      ],
      "metadata": {
        "id": "WpKe6D__uzSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim=512\n",
        "n_layers=6\n",
        "dropout=0.1\n",
        "dff=2048\n",
        "n_heads=8\n",
        "d_model=512"
      ],
      "metadata": {
        "id": "Je8e_P_auzUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer=Transformer(max_inp_len,src_vocab_size,max_tar_len, trg_vocab_size,embed_dim,n_layers,dropout,dff,n_heads,d_model)"
      ],
      "metadata": {
        "id": "eIhqpdDyuzY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "mgEYR2Auuzbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test,y_train,y_test=train_test_split(src_sequences,tar_sequences,test_size=0.010,random_state=0)"
      ],
      "metadata": {
        "id": "xBJquQYvuze6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape,X_test.shape,y_train.shape,y_test.shape"
      ],
      "metadata": {
        "id": "q41cTiW6uzhw",
        "outputId": "3ba935b1-c719-4110-d79a-bcada0b2080b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((19800, 1), (200, 1), (19800, 14), (200, 14))"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer=tf.keras.optimizers.Adam(learning_rate=0.001,beta_1=0.9,beta_2=0.98,epsilon=1e-9,clipnorm=5)"
      ],
      "metadata": {
        "id": "l2WEW1bUuzlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_loss(y_true,y_pred):\n",
        "  loss_fn=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,reduction='none')\n",
        "  loss=loss_fn(y_true,y_pred)\n",
        "  mask=tf.cast(y_true!=0,dtype=loss.dtype)\n",
        "  loss*=mask\n",
        "  return tf.reduce_sum(loss)/tf.reduce_sum(mask)"
      ],
      "metadata": {
        "id": "SYW9J9UvsZgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_accuracy(y_true,y_pred):\n",
        "  y_pred=tf.argmax(y_pred,axis=-1)\n",
        "  y_pred=tf.cast(y_pred,dtype=y_true.dtype)\n",
        "  mask=tf.cast(y_true!=0,dtype=tf.float32)\n",
        "  match=tf.cast(y_true==y_pred,dtype=tf.float32)\n",
        "  return tf.reduce_sum(match)/tf.reduce_sum(mask)"
      ],
      "metadata": {
        "id": "lWQZ2V_RyQDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "expected_loss=tf.math.log(tf.cast(trg_vocab_size,tf.float32)).numpy()\n",
        "expected_loss"
      ],
      "metadata": {
        "id": "gegsRCzEztzi",
        "outputId": "d3ad9dce-1c25-470e-b7fc-9a30ee89b1e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8.968651"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "expected_accuracy=tf.cast(1/trg_vocab_size,tf.float32).numpy()\n",
        "expected_accuracy"
      ],
      "metadata": {
        "id": "FeSfbDfS0JiL",
        "outputId": "9a861d8b-8610-4102-ae15-becec4ebdc20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.00012733987"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.compile(optimizer=optimizer,loss=masked_loss,metrics=[masked_accuracy,masked_loss])"
      ],
      "metadata": {
        "id": "L3Tv4l8JyQIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.evaluate([X_test,y_test[:,:-1]],y_test[:,1:],steps=6,return_dict=True)"
      ],
      "metadata": {
        "id": "BHLg_I6Y0fWL",
        "outputId": "166e3f14-0267-40f7-ccce-28214e472d62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6/6 [==============================] - 14s 550ms/step - loss: 8.9826 - masked_accuracy: 0.0000e+00 - masked_loss: 8.9835\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss': 8.982586860656738,\n",
              " 'masked_accuracy': 0.0,\n",
              " 'masked_loss': 8.983532905578613}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vE3r9L6DILzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history=transformer.fit([X_train,y_train[:,:-1]],y_train[:,1:],steps_per_epoch=10,epochs=10,validation_data=(\n",
        "    [X_test,y_test[:,:-1]],y_test[:,1:]),validation_steps=20,callbacks=[tf.keras.callbacks.EarlyStopping(patience=3)]\n",
        "  )"
      ],
      "metadata": {
        "id": "qkRHOFM1yQO6",
        "outputId": "dbc47402-19d3-42fc-9fd5-67581434216a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import data"
      ],
      "metadata": {
        "id": "PyEcpDPJL9mf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "train_dataset = train_dataset.batch(batch_size)"
      ],
      "metadata": {
        "id": "ZaAGJ0XGUNKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(encoder_inputs,decoder_inputs,decoder_targets):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = transformer([encoder_inputs,decoder_inputs], training=True)\n",
        "        loss = masked_loss(decoder_targets, predictions)\n",
        "        accuracy = masked_accuracy(decoder_targets, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "    return loss,accuracy"
      ],
      "metadata": {
        "id": "CAHAYAPgL9wB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time"
      ],
      "metadata": {
        "id": "Q6DdW65UVG5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import data, train, math, reduce_sum, cast, equal, argmax, float32, GradientTape, TensorSpec, function, int64"
      ],
      "metadata": {
        "id": "t8Ajld-MVRuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
        "class LRScheduler(LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000, **kwargs):\n",
        "        super(LRScheduler, self).__init__(**kwargs)\n",
        "\n",
        "        self.d_model = cast(d_model, float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step_num):\n",
        "\n",
        "        arg1 = step_num ** -0.5\n",
        "        arg2 = step_num * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return (self.d_model ** -0.5) * math.minimum(arg1, arg2)"
      ],
      "metadata": {
        "id": "Ymd2ebWMXb4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(0.01, beta_1=0.9, beta_2=0.98,epsilon=1e-9)"
      ],
      "metadata": {
        "id": "o1hY4ebHXTik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs=10\n",
        "batch_size=64\n",
        "for epoch in range(epochs):\n",
        "    print(\"\\nStart of epoch %d\" % (epoch + 1))\n",
        "    total_loss=0.0\n",
        "    total_accuracy=0.0\n",
        "    start_time = time()\n",
        "\n",
        "    # Iterate over the dataset batches\n",
        "    stepp=0\n",
        "    for step, (train_batchX, train_batchY) in enumerate(train_dataset):\n",
        "\n",
        "        encoder_input = train_batchX[:, 1:]\n",
        "        decoder_input = train_batchY[:, :-1]\n",
        "        decoder_output = train_batchY[:, 1:]\n",
        "\n",
        "        loss,accuarcy=train_step(encoder_input, decoder_input, decoder_output)\n",
        "        total_loss+=loss\n",
        "        total_accuracy+=accuracy\n",
        "\n",
        "        if step % 50 == 0:\n",
        "            print(f'Epoch {epoch + 1} Step {step} Loss {loss:.4f} Accuracy {accuracy:.4f}')\n",
        "        stepp+=1\n",
        "    print(\"Epoch %d: Training Loss %.4f, Training Accuracy %.4f\" % (epoch + 1, total_loss, total_accuracy))\n",
        "    average_loss = total_loss / stepp\n",
        "    print(f\"Epoch {epoch + 1}/{epochs} - Loss: {average_loss:.4f}\")\n",
        "\n",
        "print(\"Total time taken: %.2fs\" % (time() - start_time))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "150469zBP0C-",
        "outputId": "c870b3b3-69c6-401e-bfd7-10747e4a034b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Start of epoch 1\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}