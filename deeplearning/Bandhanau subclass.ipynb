{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b78ece05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f408d68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d544406",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input,Embedding,GRU,Bidirectional,Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15af2a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d6fabf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0eaae869",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        query,values=inputs\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "        score1=self.W1(query_with_time_axis)\n",
    "        score2=self.W2(values)\n",
    "        combined_score=tf.nn.tanh(score1 + score2)\n",
    "        score = self.V(combined_score)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector,attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a657cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x)\n",
    "        return output, state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8dde025b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self,inputs):\n",
    "        x, hidden, enc_output=inputs\n",
    "        context_vector, attention_weights = self.attention([hidden, enc_output])\n",
    "        x=tf.expand_dims(x,0)\n",
    "        x = self.embedding(x)\n",
    "        x=tf.transpose(x,perm=(1,0,2))\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        output, state = self.gru(x,initial_state=hidden)\n",
    "\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9b2f713e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(tf.keras.Model):\n",
    "    def __init__(self, i_vocab_size,o_vocab_size,embedding_dim, gru_units):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = Encoder(i_vocab_size, embedding_dim, gru_units)\n",
    "        self.decoder = Decoder(o_vocab_size, embedding_dim, gru_units)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_seq, target_seq= inputs\n",
    "        enc_output, enc_hidden = self.encoder(input_seq)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        loss = 0\n",
    "\n",
    "        for t in range(target_seq.shape[1]):\n",
    "            predictions, dec_hidden, _ = self.decoder([target_seq[:, t], dec_hidden, enc_output])\n",
    "\n",
    "            loss += tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(target_seq[:, t], predictions, \n",
    "                                                                                   from_logits=True))\n",
    "\n",
    "        batch_loss = (loss / int(target_seq.shape[1]))\n",
    "\n",
    "        return batch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0641da87",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq(src_vocab_size,trg_vocab_size,embed_size,gru_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "430a5d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=keras.optimizers.Adam(learning_rate=0.7,clipnorm=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc42462a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2da4b26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Loss: 1.3728\n",
      "Epoch 2/2, Loss: 1.1201\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS =2\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    total_loss = 0.0\n",
    "    steps_per_epoch = max_inp_len // batch_size\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = model([src_sequences,tar_sequences])\n",
    "        total_loss += loss\n",
    "\n",
    "    grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {total_loss.numpy():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "619c759c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"seq2_seq\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder (Encoder)           multiple                  187000    \n",
      "                                                                 \n",
      " decoder (Decoder)           multiple                  419426    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 606426 (2.31 MB)\n",
      "Trainable params: 606426 (2.31 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d7bd9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3864219",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7957375f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b98a5d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78b0cac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2db268a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf32f6c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b631557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f49b4778",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50d60542",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "outputs = []\n",
    "\n",
    "data_file = open('spa.txt', encoding='utf-8')\n",
    "\n",
    "count = 0\n",
    "for line in data_file:\n",
    "    count += 1\n",
    "    if count > 100:\n",
    "        break\n",
    "    if '\\t' not in line:\n",
    "        continue\n",
    "    ip, temp_op, extra = line.rstrip().split('\\t')\n",
    "    op = temp_op\n",
    "    inputs.append(ip)\n",
    "    outputs.append(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b77ec9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=[sentence.lower() for sentence in inputs]\n",
    "outputs=[sentence.lower() for sentence in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70229740",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = ['<start> '+sentence+' <end>' for sentence in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09dcd61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "EnglishTokenizer=Tokenizer(oov_token=\"<UNK>\")\n",
    "EnglishTokenizer.fit_on_texts(inputs)\n",
    "inp_sequences=EnglishTokenizer.texts_to_sequences(inputs)\n",
    "max_inp_len=max(len(i) for i in inp_sequences)\n",
    "src_sequences=pad_sequences(inp_sequences,maxlen=max_inp_len,padding=\"post\")\n",
    "Englishword2index=EnglishTokenizer.word_index\n",
    "Englishindex2word=EnglishTokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42c1da62",
   "metadata": {},
   "outputs": [],
   "source": [
    "SpanishTokenizer=Tokenizer(oov_token=\"<UNK>\")\n",
    "SpanishTokenizer.fit_on_texts(outputs)\n",
    "op_sequences=SpanishTokenizer.texts_to_sequences(outputs)\n",
    "max_tar_len=max(len(i) for i in op_sequences)\n",
    "tar_sequences=pad_sequences(op_sequences,maxlen=max_tar_len,padding=\"post\")\n",
    "Spanishword2index=SpanishTokenizer.word_index\n",
    "Spanishindex2word=SpanishTokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2990851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_vocab_size: 58\n",
      "tar_vocab_size: 125\n"
     ]
    }
   ],
   "source": [
    "src_vocab_size=len(Englishword2index)+1\n",
    "trg_vocab_size=len(Spanishword2index)+1\n",
    "print(\"src_vocab_size:\",src_vocab_size)\n",
    "print(\"tar_vocab_size:\",trg_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c311db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_inp_len: 2\n",
      "max_tar_len: 5\n"
     ]
    }
   ],
   "source": [
    "print(\"max_inp_len:\",max_inp_len)\n",
    "print(\"max_tar_len:\",max_tar_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ec69454",
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_units=200\n",
    "embed_size=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff593419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 5)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "458b6641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51098758",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.random.random((2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2625bece",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_generator(\n",
    "    data_generator,\n",
    "    args=(src_sequences,tar_sequences, batch_size),\n",
    "    output_types=(tf.int32, tf.int32),\n",
    "    output_shapes=(\n",
    "        tf.TensorShape((batch_size, src_sequences.shape[1])),\n",
    "        tf.TensorShape((batch_size, tar_sequences.shape[1]))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc9f647e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e68f397b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(encoder_input_data, decoder_input_data, batch_size):\n",
    "    dataset_length = len(encoder_input_data)\n",
    "    steps_per_epoch = dataset_length // batch_size\n",
    "\n",
    "    while True:\n",
    "        for i in range(steps_per_epoch):\n",
    "            encoder_batch = encoder_input_data[i * batch_size: (i + 1) * batch_size]\n",
    "            decoder_batch = decoder_input_data[i * batch_size: (i + 1) * batch_size]\n",
    "\n",
    "            yield (encoder_batch, decoder_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63273b50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1bc26640",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
