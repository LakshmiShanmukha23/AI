{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c245e0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba9d2b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6419e2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06c844f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "891c4a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding,Dense,Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525549ac",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f642050a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, hidden_dim):\n",
    "        super(PositionEmbedding, self).__init__()\n",
    "        word_embedding_matrix = self.get_position_encoding(vocab_size, hidden_dim)   \n",
    "        position_embedding_matrix = self.get_position_encoding(sequence_length, hidden_dim)                                          \n",
    "        self.word_embedding_layer = Embedding(\n",
    "            input_dim=vocab_size, output_dim=hidden_dim,\n",
    "            weights=[word_embedding_matrix],\n",
    "            trainable=False\n",
    "        )\n",
    "        self.position_embedding_layer = Embedding(\n",
    "            input_dim=sequence_length, output_dim=hidden_dim,\n",
    "            weights=[position_embedding_matrix],\n",
    "            trainable=False\n",
    "        )\n",
    "             \n",
    "    def get_position_encoding(self, seq_len, d, n=10000):\n",
    "        P = np.zeros((seq_len, d))\n",
    "        for k in range(seq_len):\n",
    "            for i in np.arange(int(d/2)):\n",
    "                denominator = np.power(n, 2*i/d)\n",
    "                P[k, 2*i] = np.sin(k/denominator)\n",
    "                P[k, 2*i+1] = np.cos(k/denominator)\n",
    "        return P\n",
    " \n",
    " \n",
    "    def call(self, inputs):        \n",
    "        position_indices = tf.range(tf.shape(inputs)[-1])\n",
    "        embedded_words = self.word_embedding_layer(inputs)\n",
    "        embedded_indices = self.position_embedding_layer(position_indices)\n",
    "        return embedded_words + embedded_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03061026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q,k,v,mask=None):\n",
    "    scale=q.shape[-1]\n",
    "    k=tf.transpose(k,perm=(0,1,3,2))\n",
    "    scaled=tf.matmul(q,k)/np.sqrt(scale)\n",
    "    if mask is not None:\n",
    "        scaled+=mask\n",
    "    attention_wts=tf.nn.softmax(scaled,axis=-1)\n",
    "    values=tf.matmul(scaled,v)\n",
    "    return values,attention_wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffd1e67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHead_Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self,no_of_heads,d_model):\n",
    "        super(MultiHead_Attention,self).__init__()\n",
    "        self.n_heads=no_of_heads\n",
    "        self.head_dims=d_model//no_of_heads\n",
    "        self.qkv=Dense(3*d_model)\n",
    "        self.dense=Dense(d_model)\n",
    "        \n",
    "    def call(self,inputs,mask=None):\n",
    "        batch_size,max_inp_len,d_model=inputs.shape\n",
    "        qkv=self.qkv(inputs)\n",
    "        desired_shape = (batch_size, max_inp_len, self.n_heads, 3 * self.head_dims)\n",
    "        qkv=tf.reshape(qkv,desired_shape)\n",
    "        qkv=tf.transpose(qkv,perm=(0,2,1,3))\n",
    "        q,k,v=tf.split(qkv,num_or_size_splits=3,axis=3)\n",
    "        v,attention_wts=scaled_dot_product(q,k,v)\n",
    "        v=tf.reshape(v,(batch_size,max_inp_len,self.n_heads*self.head_dims))\n",
    "        output=self.dense(v)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80a384c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self,d_model,ffc,dropout):\n",
    "        super(FeedForward,self).__init__()\n",
    "        self.dense1=Dense(ffc,activation=\"relu\")\n",
    "        self.dense2=Dense(d_model)\n",
    "        self.dropout=Dropout(rate=dropout)\n",
    "        \n",
    "    def call(self,inputs,training=False):\n",
    "        x=self.dense1(inputs)\n",
    "        x=self.dropout(x,training=training)\n",
    "        x=self.dense2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c81b31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(tf.keras.layers.Layer):\n",
    "    def __init__(self, parameter_shape, eps=1e-5):\n",
    "        super(LayerNormalization, self).__init__()\n",
    "        self.parameter_shape = parameter_shape\n",
    "        self.eps = eps\n",
    "        self.gamma = self.add_weight(\"gamma\", shape=parameter_shape, initializer=\"ones\", trainable=True)\n",
    "        self.beta = self.add_weight(\"beta\", shape=parameter_shape, initializer=\"zeros\", trainable=True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        mean = tf.reduce_mean(inputs, axis=-1, keepdims=True)\n",
    "        var = tf.reduce_mean(tf.square(inputs - mean), axis=-1, keepdims=True)\n",
    "        std = tf.sqrt(var + self.eps)\n",
    "        y = (inputs - mean) / std\n",
    "        out = self.gamma * y + self.beta\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a255756",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_Layer(tf.keras.layers.Layer):\n",
    "    def __init__(self,num_of_heads,d_model,dff,dropout):\n",
    "        super(Encoder_Layer,self).__init__()\n",
    "        self.mha=MultiHead_Attention(num_of_heads,d_model)\n",
    "        self.dropout1=Dropout(rate=dropout)\n",
    "        self.ln1=LayerNormalization(d_model)\n",
    "        self.ff=FeedForward(d_model,dff,dropout)\n",
    "        self.dropout2=Dropout(rate=dropout)\n",
    "        self.ln2=LayerNormalization(d_model)\n",
    "        \n",
    "    def call(self,inputs,mask,training):\n",
    "        mha_output=self.mha(inputs,mask)\n",
    "        output_dropout1=self.dropout1(mha_output,training=training)\n",
    "        ln1_output=self.ln1(output_dropout1+inputs)\n",
    "        \n",
    "        ffn_output=self.ff(ln1_output,training)\n",
    "        ffn_output_dropout=self.dropout2(ffn_output,training=training)\n",
    "        ln2_output=self.ln2(ffn_output_dropout+ln1_output)\n",
    "        return ln2_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b176d7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self,inp_seq_len, inp_vocab_size,embed_dim,num_of_heads,d_model,dff,dropout,num_layers):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.embedding_layer = PositionEmbedding(inp_seq_len, inp_vocab_size,embed_dim)\n",
    "        self.encoder_layer=[Encoder_Layer(num_of_heads,d_model,dff,dropout) for _ in range(num_layers)]\n",
    "        \n",
    "    def call(self,inputs,mask=False,training=False):\n",
    "        x=self.embedding_layer(inputs)\n",
    "        for layer in self.encoder_layer:\n",
    "            x=layer(x,mask,training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a856c72a",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "712143f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedMultiHead_Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self,no_of_heads,d_model):\n",
    "        super(MaskedMultiHead_Attention,self).__init__()\n",
    "        self.n_heads=no_of_heads\n",
    "        self.head_dims=d_model//no_of_heads\n",
    "        self.qkv=Dense(3*d_model)\n",
    "        self.dense=Dense(d_model)\n",
    "        \n",
    "    def call(self,inputs,mask=None):\n",
    "        batch_size,max_inp_len,d_model=inputs.shape\n",
    "        qkv=self.qkv(inputs)\n",
    "        desired_shape = (batch_size, max_inp_len, self.n_heads, 3 * self.head_dims)\n",
    "        qkv=tf.reshape(qkv,desired_shape)\n",
    "        qkv=tf.transpose(qkv,perm=(0,2,1,3))\n",
    "        q,k,v=tf.split(qkv,num_or_size_splits=3,axis=3)\n",
    "        v,attention_wts=scaled_dot_product(q,k,v,mask)\n",
    "        v=tf.reshape(v,(batch_size,max_inp_len,self.n_heads*self.head_dims))\n",
    "        output=self.dense(v)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e6308d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadCross_Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self,no_of_heads,d_model):\n",
    "        super(MultiHeadCross_Attention,self).__init__()\n",
    "        self.n_heads=no_of_heads\n",
    "        self.head_dims=d_model//no_of_heads\n",
    "        self.kv=Dense(2*d_model)\n",
    "        self.q=Dense(d_model)\n",
    "        self.dense=Dense(d_model)\n",
    "        \n",
    "    def call(self,x,y,mask=None):\n",
    "        batch_size,max_inp_len,d_model=x.shape\n",
    "        batch_size,max_tar_len,d_model=y.shape\n",
    "        kv=self.kv(x)\n",
    "        q=self.q(y)\n",
    "        kv=tf.reshape(kv,(batch_size, max_inp_len, self.n_heads, 2 * self.head_dims))\n",
    "        q=tf.reshape(q,(batch_size, max_tar_len, self.n_heads, self.head_dims))\n",
    "        kv=tf.transpose(kv,perm=(0,2,1,3))\n",
    "        q=tf.transpose(q,perm=(0,2,1,3))\n",
    "        k,v=tf.split(kv,num_or_size_splits=2,axis=3)\n",
    "        v,attention_wts=scaled_dot_product(q,k,v,mask)\n",
    "        v=tf.reshape(v,(batch_size,max_tar_len,d_model))\n",
    "        output=self.dense(v)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0bca71cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder_Layer(tf.keras.layers.Layer):\n",
    "    def __init__(self,d_model,dropout,no_heads,dff):\n",
    "        super(Decoder_Layer,self).__init__()\n",
    "        self.mha=MaskedMultiHead_Attention(no_heads,d_model)\n",
    "        self.dropout1=Dropout(rate=dropout)\n",
    "        self.norm1=LayerNormalization(d_model)\n",
    "        \n",
    "        self.mhca=MultiHeadCross_Attention(no_heads,d_model)\n",
    "        self.dropout2=Dropout(rate=dropout)\n",
    "        self.norm2=LayerNormalization(d_model)\n",
    "        \n",
    "        self.ff=FeedForward(d_model,dff,dropout)\n",
    "        self.dropout3=Dropout(rate=dropout)\n",
    "        self.norm3=LayerNormalization(d_model)\n",
    "        \n",
    "    def call(self,x,y,look_ahead_mask,padding_mask,training=False):\n",
    "        mha_output=self.mha(y,mask=look_ahead_mask)\n",
    "        mha_output_dropout=self.dropout1(mha_output,training=training)\n",
    "        norm1_output=self.norm1(mha_output_dropout+y)\n",
    "        \n",
    "        cmha_output=self.mhca(x,norm1_output,mask=padding_mask)\n",
    "        cmha_output_dropout=self.dropout2(cmha_output,training=training)\n",
    "        norm2_output=self.norm2(cmha_output_dropout+norm1_output)\n",
    "        \n",
    "        ff_output=self.ff(norm2_output,training)\n",
    "        ff_output_dropout=self.dropout3(ff_output,training=training)\n",
    "        norm3_output=self.norm3(ff_output_dropout+norm2_output)\n",
    "        return norm3_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6e6fa3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self,tar_seq_len, tar_vocab_size,embed_dim,n_layers,d_model,dropout,no_heads,dff):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.embedding_layer = PositionEmbedding(tar_seq_len, tar_vocab_size,embed_dim)\n",
    "        self.decoder_layers=[Decoder_Layer(d_model,dropout,no_heads,dff) for _ in range(n_layers)]\n",
    "        \n",
    "    def call(self,x,y,look_ahead_mask=None,padding_mask=None,training=False):\n",
    "        y=self.embedding_layer(y)\n",
    "        for layer in self.decoder_layers:\n",
    "            y=layer(x,y,look_ahead_mask,padding_mask,training)\n",
    "        return y         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7ef78c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,inp_seq_len,inp_vocab_size,tar_seq_len, tar_vocab_size,embed_dim,n_layers,dropout,dff,n_heads,d_model):\n",
    "        super(Transformer,self).__init__()\n",
    "        self.encoder=Encoder(inp_seq_len, inp_vocab_size,embed_dim,n_heads,d_model,dff,dropout,n_layers)\n",
    "        self.decoder=Decoder(tar_seq_len, tar_vocab_size,embed_dim,n_layers,d_model,dropout,n_heads,dff)\n",
    "        self.dense=Dense(tar_vocab_size)\n",
    "        \n",
    "    def create_padding_mask(self,seq):\n",
    "        seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "        return seq[:,tf.newaxis,tf.newaxis,:]\n",
    "    \n",
    "    def create_look_ahead_mask(self,size):\n",
    "        mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "        return mask\n",
    "    \n",
    "    def create_masks(self,inputs, target):\n",
    "        enc_padding_mask = self.create_padding_mask(inputs)\n",
    "        dec_padding_mask = self.create_padding_mask(target)\n",
    "        look_ahead_mask = self.create_look_ahead_mask(tf.shape(target)[1])\n",
    "        look_ahead_mask = tf.maximum(dec_padding_mask, look_ahead_mask)\n",
    "\n",
    "        return enc_padding_mask, look_ahead_mask, dec_padding_mask\n",
    "    \n",
    "    def call(self,inputs,training=False):\n",
    "        x,y=inputs\n",
    "        enc_padding_mask,lookahead_mask,dec_padding_mask=self.create_masks(x,y)\n",
    "        print(enc_padding_mask.shape)\n",
    "        print(dec_padding_mask.shape)\n",
    "        print(lookahead_mask.shape)\n",
    "        encoder_op=self.encoder(x,mask=enc_padding_mask,training=training)\n",
    "        decoder_op=self.decoder(encoder_op,y,look_ahead_mask=lookahead_mask,\n",
    "                            padding_mask=enc_padding_mask,training=training)\n",
    "        output=self.dense(decoder_op)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b1b8f95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_seq_len=10\n",
    "tar_seq_len=15\n",
    "inp_vocab_size=50\n",
    "tar_vocab_size=100\n",
    "embed_dim=512\n",
    "n_layers=6\n",
    "dropout=0.1\n",
    "dff=2048\n",
    "n_heads=8\n",
    "d_model=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "60255ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer=Transformer(inp_seq_len,inp_vocab_size,tar_seq_len, tar_vocab_size,embed_dim,n_layers,dropout,dff,n_heads,d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3daf62eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.random.random((150,10))\n",
    "y=np.random.random((150,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c08ec9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 1, 1, 10)\n",
      "(150, 1, 1, 15)\n",
      "(150, 1, 15, 15)\n"
     ]
    }
   ],
   "source": [
    "output=transformer([x,y],True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "2336c5f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([150, 15, 100])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "cd0ccf17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_18 (Encoder)        multiple                  18945024  \n",
      "                                                                 \n",
      " decoder_18 (Decoder)        multiple                  25283072  \n",
      "                                                                 \n",
      " dense_501 (Dense)           multiple                  51300     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44279396 (168.91 MB)\n",
      "Trainable params: 44189796 (168.57 MB)\n",
      "Non-trainable params: 89600 (350.00 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620922fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
