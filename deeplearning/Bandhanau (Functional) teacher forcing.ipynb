{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98969ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "218773d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51c67d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import GRU,Embedding,Dense,Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "497e107d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dc7f1b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        query,values=inputs\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "        score1=self.W1(query_with_time_axis)\n",
    "        score2=self.W2(values)\n",
    "        combined_score=tf.nn.tanh(score1 + score2)\n",
    "        score = self.V(combined_score)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b2985590",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs=Input(shape=(max_inp_len,))\n",
    "decoder_inputs=Input(shape=(1,))\n",
    "\n",
    "encoder_embedded=Embedding(src_vocab_size,embedding_dim)\n",
    "decoder_embedded=Embedding(trg_vocab_size,embedding_dim)\n",
    "\n",
    "encoder_embed=encoder_embedded(encoder_inputs)\n",
    "\n",
    "decoder_embed=decoder_embedded(decoder_inputs)\n",
    "\n",
    "encoder_gru=GRU(gru_units,return_state=True,return_sequences=True,recurrent_initializer='glorot_uniform')\n",
    "encoder_op,hidden=encoder_gru(encoder_embed)\n",
    "\n",
    "attention=BahdanauAttention(gru_units)\n",
    "context_vector=attention([hidden,encoder_op])\n",
    "decoder_embed = tf.concat([tf.expand_dims(context_vector, 1), decoder_embed], axis=-1)\n",
    "decoder_gru=GRU(gru_units,return_state=True,return_sequences=True,recurrent_initializer='glorot_uniform')\n",
    "output,h1=decoder_gru(decoder_embed,initial_state=hidden)\n",
    "output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "decoder_fc=Dense(trg_vocab_size,activation=\"softmax\")\n",
    "decoder_op=decoder_fc(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "895c9520",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model=Model(encoder_inputs,[encoder_op,hidden])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b371cfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_model=Model([decoder_inputs,encoder_op,hidden],[decoder_op,h1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6dd74133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_19 (InputLayer)       [(None, 5)]               0         \n",
      "                                                                 \n",
      " embedding_8 (Embedding)     (None, 5, 150)            208800    \n",
      "                                                                 \n",
      " gru_8 (GRU)                 [(None, 5, 100),          75600     \n",
      "                              (None, 100)]                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 284400 (1.08 MB)\n",
      "Trainable params: 284400 (1.08 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9ffaad74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_22 (InputLayer)       [(None, 100)]                0         []                            \n",
      "                                                                                                  \n",
      " input_21 (InputLayer)       [(None, 5, 100)]             0         []                            \n",
      "                                                                                                  \n",
      " bahdanau_attention_4 (Bahd  (None, 100)                  20301     ['input_22[0][0]',            \n",
      " anauAttention)                                                      'input_21[0][0]']            \n",
      "                                                                                                  \n",
      " input_20 (InputLayer)       [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " tf.expand_dims_4 (TFOpLamb  (None, 1, 100)               0         ['bahdanau_attention_4[1][0]']\n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " embedding_9 (Embedding)     (None, 1, 150)               465900    ['input_20[0][0]']            \n",
      "                                                                                                  \n",
      " tf.concat_4 (TFOpLambda)    (None, 1, 250)               0         ['tf.expand_dims_4[1][0]',    \n",
      "                                                                     'embedding_9[1][0]']         \n",
      "                                                                                                  \n",
      " gru_9 (GRU)                 [(None, 1, 100),             105600    ['tf.concat_4[1][0]',         \n",
      "                              (None, 100)]                           'input_22[0][0]']            \n",
      "                                                                                                  \n",
      " tf.reshape_4 (TFOpLambda)   (None, 100)                  0         ['gru_9[1][0]']               \n",
      "                                                                                                  \n",
      " dense_17 (Dense)            (None, 3106)                 313706    ['tf.reshape_4[1][0]']        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 905507 (3.45 MB)\n",
      "Trainable params: 905507 (3.45 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "885f93dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=keras.optimizers.Adam(learning_rate=0.7,clipnorm=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0882b090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(inputs):\n",
    "    input_seq, target_seq= inputs\n",
    "    input_target_seq=target_seq[:,:-1]\n",
    "    target_target_seq=target_seq[:,1:]\n",
    "    enc_output, enc_hidden = encoder_model(input_seq)\n",
    "    dec_hidden = enc_hidden\n",
    "    loss = 0\n",
    "    for t in range(target_seq.shape[1]-1):\n",
    "        predictions, dec_hidden = decoder_model([input_target_seq[:, t], enc_output, dec_hidden])\n",
    "        loss += tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(target_target_seq[:, t], predictions, \n",
    "                                                                               from_logits=False))\n",
    "        batch_loss = (loss / int(target_seq.shape[1]))\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ed72486d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 18.1980\n",
      "Epoch 2/5, Loss: 8.4369\n",
      "Epoch 3/5, Loss: 7.7339\n",
      "Epoch 4/5, Loss: 12.4335\n",
      "Epoch 5/5, Loss: 16.7369\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS =5\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    total_loss = 0.0\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = train_model([src_sequences,tar_sequences])\n",
    "        total_loss += loss\n",
    "\n",
    "    grads = tape.gradient(total_loss, encoder_model.trainable_variables + decoder_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, encoder_model.trainable_variables + decoder_model.trainable_variables))\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {total_loss.numpy():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "124d6a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ba997e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "outputs = []\n",
    "\n",
    "data_file = open('spa.txt', encoding='utf-8')\n",
    "\n",
    "count = 0\n",
    "for line in data_file:\n",
    "    count += 1\n",
    "    if count > 5000:\n",
    "        break\n",
    "    if '\\t' not in line:\n",
    "        continue\n",
    "    ip, temp_op, extra = line.rstrip().split('\\t')\n",
    "    op = temp_op\n",
    "    inputs.append(ip)\n",
    "    outputs.append(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4ba3c538",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=[sentence.lower() for sentence in inputs]\n",
    "outputs=[sentence.lower() for sentence in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "234707f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = ['<start> '+sentence+' <end>' for sentence in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1e39d152",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=150\n",
    "gru_units=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1cdaf799",
   "metadata": {},
   "outputs": [],
   "source": [
    "EnglishTokenizer=Tokenizer(oov_token=\"<UNK>\")\n",
    "EnglishTokenizer.fit_on_texts(inputs)\n",
    "inp_sequences=EnglishTokenizer.texts_to_sequences(inputs)\n",
    "max_inp_len=max(len(i) for i in inp_sequences)\n",
    "src_sequences=pad_sequences(inp_sequences,maxlen=max_inp_len,padding=\"post\")\n",
    "Englishword2index=EnglishTokenizer.word_index\n",
    "Englishindex2word=EnglishTokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "429f7f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "SpanishTokenizer=Tokenizer(oov_token=\"<UNK>\")\n",
    "SpanishTokenizer.fit_on_texts(outputs)\n",
    "op_sequences=SpanishTokenizer.texts_to_sequences(outputs)\n",
    "max_tar_len=max(len(i) for i in op_sequences)\n",
    "tar_sequences=pad_sequences(op_sequences,maxlen=max_tar_len,padding=\"post\")\n",
    "Spanishword2index=SpanishTokenizer.word_index\n",
    "Spanishindex2word=SpanishTokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6f6e0c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_vocab_size: 1392\n",
      "tar_vocab_size: 3106\n"
     ]
    }
   ],
   "source": [
    "src_vocab_size=len(Englishword2index)+1\n",
    "trg_vocab_size=len(Spanishword2index)+1\n",
    "print(\"src_vocab_size:\",src_vocab_size)\n",
    "print(\"tar_vocab_size:\",trg_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c1cd50c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_inp_len: 5\n",
      "max_tar_len: 10\n"
     ]
    }
   ],
   "source": [
    "print(\"max_inp_len:\",max_inp_len)\n",
    "print(\"max_tar_len:\",max_tar_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "211eb1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 5)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4eaa2acc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 10)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597068b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
