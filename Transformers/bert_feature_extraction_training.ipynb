{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10dac241",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29db961c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43cd95d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer,BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a034ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer=BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model=BertForMaskedLM.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5e55fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"clean.txt\",'r') as fp:\n",
    "    text=fp.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6a1223b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['From my grandfather Verus I learned good morals and the government of my temper.',\n",
       " 'From the reputation and remembrance of my father, modesty and a manly character.',\n",
       " 'From my mother, piety and beneficence, and abstinence, not only from evil deeds, but even from evil thoughts; and further, simplicity in my way of living, far removed from the habits of the rich.',\n",
       " 'From my great-grandfather, not to have frequented public schools, and to have had good teachers at home, and to know that on such things a man should spend liberally.',\n",
       " \"From my governor, to be neither of the green nor of the blue party at the games in the Circus, nor a partizan either of the Parmularius or the Scutarius at the gladiators' fights; from him too I learned endurance of labour, and to want little, and to work with my own hands, and not to meddle with other people's affairs, and not to be ready to listen to slander.\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "135db0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=tokenizer(text[:8],return_tensors='pt',padding=True,truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13a48eeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 240])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5aa0dc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[\"label\"]=inputs.input_ids.detach().clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cec5f070",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand=torch.rand(inputs.input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4617fa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_arr=(rand<0.15)*(inputs.input_ids!=101)*(inputs.input_ids!=102)*(inputs.input_ids!=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d77b2a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        ...,\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False,  True,  ..., False, False, False]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "857cfa59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 240])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0625bd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "selection=[]\n",
    "for i in range(len(mask_arr)):\n",
    "    selection.append(torch.flatten(mask_arr[i].nonzero()).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "04d57592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 10], [16]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selection[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3f62e10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(mask_arr)):\n",
    "    inputs.input_ids[i,selection[i]]=103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f31a9be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2bd0104d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataSet(Dataset):\n",
    "    def __init__(self,encodings):\n",
    "        self.encodings=encodings\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.encodings.input_ids.shape[0]\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        sample={\n",
    "            key: torch.tensor(val[idx]) for key,val in self.encodings.items()   \n",
    "        }\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7b3a8cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=CustomDataSet(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f623b5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3e4caeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=DataLoader(dataset,batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b3934d6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cdd3b989",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim=torch.optim.AdamW(model.parameters(),lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "77db0918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7ffad291",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/4 [00:00<?, ?it/s]C:\\Users\\Rambabu\\AppData\\Local\\Temp\\ipykernel_6408\\4052576561.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  key: torch.tensor(val[idx]) for key,val in self.encodings.items()\n",
      "Epoch 0: 100%|████████████████████████████████████████████████████████████████| 4/4 [00:53<00:00, 13.31s/it, loss=15.6]\n",
      "Epoch 1: 100%|████████████████████████████████████████████████████████████████| 4/4 [00:45<00:00, 11.38s/it, loss=9.31]\n"
     ]
    }
   ],
   "source": [
    "epochs=2\n",
    "for epoch in range(epochs):\n",
    "    loop=tqdm(loader,leave=True)\n",
    "    for batch in loop:\n",
    "        optim.zero_grad()\n",
    "        input_ids=batch[\"input_ids\"]\n",
    "        attention_mask=batch[\"attention_mask\"]\n",
    "        labels=batch[\"label\"]\n",
    "        outputs=model(input_ids,attention_mask=attention_mask,labels=labels)\n",
    "        loss=outputs.loss\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        loop.set_description(f\"Epoch {epoch}\")\n",
    "        loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "50f3f966",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    op=model(inputs.input_ids,attention_mask=inputs.attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a8ae97b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits'])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b2f75d2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 240, 30522])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "38f9447f",
   "metadata": {},
   "outputs": [],
   "source": [
    "score=torch.nn.functional.softmax(op.logits,dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ac9e74fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=torch.argmax(score,dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "001a4b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual=inputs.label[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "503cd73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre=tokens[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0e05c22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "act=tokenizer.decode(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b7018ca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] from my great - grandfather, not to have frequented public schools, and to have had good teachers at home, and to know that on such things a man should spend liberally. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e67e4666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'. from the great - grandfather to not to have attended public schools, and to have had good teachers at home, and to know that on such things a man should spend liberally ;. and and money ; education not been not not,,,,,, that childhood learning learning learning age should grandfather should should to to to. private, but not have been been teachers good,,,,, learning all such occasions occasions time should spend education to to to should. good good extensively not not not have been good,,,, and to learning all all such subjects should money should liberal.ly. \" in and. to to from the education man not liberal grandfather,, to to necessity that all these childhood all good subjects grandfather should liberal grandfather to. to to only good public extensively but but have have been good teachers good,,, not also that all these education learning the education man grandfather to should liberal liberal extensively. to and and ; good all education education such good been been had,,, fairly,,, also all learning education all account should not liberal money. should. ; and all and education education education the money ;. should more liberal... and and to'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_pred=tokenizer.decode(pre)\n",
    "new_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5996f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
